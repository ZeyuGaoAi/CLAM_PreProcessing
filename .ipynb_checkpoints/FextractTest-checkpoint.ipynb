{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e8bdf0e-5992-456f-93f7-0c4c258f5f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/z/zeyugao/pyvenv/smmile_new/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import argparse\n",
    "import pdb\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import openslide\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from utils.file_utils import save_hdf5\n",
    "from dataset_modules.dataset_h5 import Dataset_All_Bags, Whole_Slide_Bag_FP\n",
    "from models import get_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec1fbcea-903b-4efe-9548-57dedffa76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfe6ad48-c9a5-4e9e-a1f3-c44c91070ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    data_h5_dir='/home/z/zeyugao/dataset/WSIData/Camelyon/clam_gen/',        # 设置你的数据 H5 文件夹路径\n",
    "    anno_dir='/home/shared/su123/Camelyon/annotation/',              # 设置你的注释文件夹路径\n",
    "    data_slide_dir='/home/shared/su123/Camelyon/WSIs/',  # 设置你的切片数据文件夹路径\n",
    "    slide_ext='.tif',                      \n",
    "    # 设置文件扩展名\n",
    "    csv_path='/home/z/zeyugao/dataset/WSIData/Camelyon/clam_gen/process_list_autogen.csv',          # 设置 CSV 文件路径\n",
    "    feat_dir='/home/z/zeyugao/dataset/WSIData/Camelyon/clam_gen/dsmil',              # 设置特征输出文件夹路径\n",
    "    model_name='dsmil_camel',              # 选择的模型名称\n",
    "    batch_size=128,                           # 批处理大小\n",
    "    no_auto_skip=False,                       # 是否自动跳过\n",
    "    target_patch_size=224,                    # 目标 patch 大小\n",
    "    suffix=\"_0_512\",                           # 文件名后缀\n",
    "    patch_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "694ffd30-1fb8-44a2-886a-55ac808eb858",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = args.csv_path\n",
    "bags_dataset = Dataset_All_Bags(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f115d22-70f9-4f91-bb2a-f2dae3cb76d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(args.feat_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "024ec718-28e8-4449-9865-3f5aeae8f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_files = os.listdir(args.feat_dir)\n",
    "anno_list = glob(os.path.join(args.anno_dir, '*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96ee0499-af14-403c-ba21-431ec11ab0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/z/zeyugao/pyvenv/smmile_new/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/gpfs/home/z/zeyugao/pyvenv/smmile_new/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IClassifier(\n",
      "  (feature_extractor): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model, img_transforms = get_encoder(args.model_name, target_img_size=args.target_patch_size)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65aaf890-cbf5-4481-8917-ce3829f9cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "total = len(bags_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26ad8afb-74e1-44de-a238-a342b59492ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_kwargs = {'num_workers': 8, 'pin_memory': True} if device.type == \"cuda\" else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d11f19f-40f9-45b1-a687-fbdc2bd3dad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/399 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/gpfs/home/shared/su123/Camelyon/WSIs/c16/normal_001.tif.svs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenSlideUnsupportedFormatError\u001b[0m           Traceback (most recent call last)",
      "File \u001b[0;32m/gpfs/home/z/zeyugao/pyvenv/smmile_new/lib/python3.10/site-packages/openslide/__init__.py:463\u001b[0m, in \u001b[0;36mopen_slide\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOpenSlide\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OpenSlideUnsupportedFormatError:\n",
      "File \u001b[0;32m/gpfs/home/z/zeyugao/pyvenv/smmile_new/lib/python3.10/site-packages/openslide/__init__.py:179\u001b[0m, in \u001b[0;36mOpenSlide.__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_osr \u001b[38;5;241m=\u001b[39m \u001b[43mlowlevel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lowlevel\u001b[38;5;241m.\u001b[39mread_icc_profile\u001b[38;5;241m.\u001b[39mavailable:\n",
      "File \u001b[0;32m/gpfs/home/z/zeyugao/pyvenv/smmile_new/lib/python3.10/site-packages/openslide/lowlevel.py:203\u001b[0m, in \u001b[0;36m_check_open\u001b[0;34m(result, _func, _args)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenSlideUnsupportedFormatError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported or missing image file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    204\u001b[0m slide \u001b[38;5;241m=\u001b[39m _OpenSlide(c_void_p(result))\n",
      "\u001b[0;31mOpenSlideUnsupportedFormatError\u001b[0m: Unsupported or missing image file",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     anno_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     11\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mfeat_dir, slide_id \u001b[38;5;241m+\u001b[39m args\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m wsi \u001b[38;5;241m=\u001b[39m \u001b[43mopenslide\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_slide\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslide_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Whole_Slide_Bag_FP(file_path\u001b[38;5;241m=\u001b[39mh5_file_path,\n\u001b[1;32m     14\u001b[0m                              anno_path\u001b[38;5;241m=\u001b[39manno_path,\n\u001b[1;32m     15\u001b[0m                              wsi\u001b[38;5;241m=\u001b[39mwsi,\n\u001b[1;32m     16\u001b[0m                              ori_patch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mpatch_size,\n\u001b[1;32m     17\u001b[0m                              img_transforms\u001b[38;5;241m=\u001b[39mimg_transforms)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/home/z/zeyugao/pyvenv/smmile_new/lib/python3.10/site-packages/openslide/__init__.py:465\u001b[0m, in \u001b[0;36mopen_slide\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OpenSlide(filename)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OpenSlideUnsupportedFormatError:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImageSlide\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/home/z/zeyugao/pyvenv/smmile_new/lib/python3.10/site-packages/openslide/__init__.py:353\u001b[0m, in \u001b[0;36mImageSlide.__init__\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124micc_profile\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/gpfs/home/z/zeyugao/pyvenv/smmile_new/lib/python3.10/site-packages/PIL/Image.py:3277\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3274\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3277\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3278\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/gpfs/home/shared/su123/Camelyon/WSIs/c16/normal_001.tif.svs'"
     ]
    }
   ],
   "source": [
    "# bag_candidate_idx = 100\n",
    "for bag_candidate_idx in tqdm(range(total)):\n",
    "    slide_id = os.path.basename(bags_dataset[bag_candidate_idx]).split(args.slide_ext)[0]\n",
    "    uuid = bags_dataset[bag_candidate_idx].split('/')[-2]\n",
    "    bag_name = slide_id+'.h5'\n",
    "    h5_file_path = os.path.join(args.data_h5_dir, 'patches', bag_name)\n",
    "    slide_file_path = os.path.join(args.data_slide_dir, uuid, slide_id + args.slide_ext)\n",
    "    anno_path = args.anno_dir + '%s.png' % (slide_id)\n",
    "    if anno_path not in anno_list:\n",
    "        anno_path = None\n",
    "    output_path = os.path.join(args.feat_dir, slide_id + args.suffix + '.npy')\n",
    "    wsi = openslide.open_slide(slide_file_path)\n",
    "    dataset = Whole_Slide_Bag_FP(file_path=h5_file_path,\n",
    "                                 anno_path=anno_path,\n",
    "                                 wsi=wsi,\n",
    "                                 ori_patch_size=args.patch_size,\n",
    "                                 img_transforms=img_transforms)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "009f494b-8c13-455a-9cc2-a8d491877a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset=dataset, batch_size=args.batch_size, **loader_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "366e0bd8-d99b-452d-b342-6a1c5afb595e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "features_list = []\n",
    "indexs_list = []\n",
    "inst_labels_list = []\n",
    "for count, data in enumerate(tqdm(loader)):\n",
    "    with torch.inference_mode():\t\n",
    "        batch = data['img']\n",
    "        coords = data['coord']\n",
    "        inst_labels = data['inst_label'].tolist()\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        \n",
    "        features = model(batch)\n",
    "        features = features.cpu().numpy().astype(np.float32)\n",
    "\n",
    "        features_list.append(features)\n",
    "        indexs_list += coords\n",
    "        inst_labels_list += inst_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7de2194-0289-4963-9172-305f9ea8a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dsmil \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from collections import OrderedDict\n",
    "\n",
    "class IClassifier(nn.Module):\n",
    "    def __init__(self, feature_extractor, feature_size, output_class):\n",
    "        super(IClassifier, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = feature_extractor      \n",
    "        self.fc = nn.Linear(feature_size, output_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        feats = self.feature_extractor(x) # N x K\n",
    "        # c = self.fc(feats.view(feats.shape[0], -1)) # N x C\n",
    "        return feats.view(feats.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91d773e6-8757-4fff-b0a5-f81d50ce816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm=nn.InstanceNorm2d\n",
    "resnet = models.resnet18(pretrained=False, norm_layer=norm)\n",
    "resnet.fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "454141dc-ba54-4aba-9b1f-cf863651a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_classifier = IClassifier(resnet, 512, output_class=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0dc5a721-6d58-4301-82e7-eb0a294bdb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['fc.weight', 'fc.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_path = \"/home/z/zeyugao/PreModel/dsmil/model-v1-lung.pth\"\n",
    "\n",
    "state_dict_weights = torch.load(weight_path)\n",
    "for i in range(4):\n",
    "    state_dict_weights.popitem()\n",
    "state_dict_init = i_classifier.state_dict()\n",
    "new_state_dict = OrderedDict()\n",
    "for (k, v), (k_0, v_0) in zip(state_dict_weights.items(), state_dict_init.items()):\n",
    "    name = k_0\n",
    "    new_state_dict[name] = v\n",
    "i_classifier.load_state_dict(new_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4075660d-9606-4090-8ba8-08f819d33bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "\n",
    "# 创建一个224x224的随机RGB图像\n",
    "random_image_array = np.random.randint(0, 256, (224, 224, 3), dtype=np.uint8)\n",
    "random_image = Image.fromarray(random_image_array)\n",
    "transform = Compose([\n",
    "    ToTensor()  # 将PIL图像转换为张量\n",
    "])\n",
    "\n",
    "# 应用转换\n",
    "transformed_image = transform(random_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e52b46b8-7bf7-4b27-a780-bd689b8db7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_image = transformed_image.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "498ccaf3-3553-45f9-af01-1f5a15f2bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = transformed_image.float().cuda()\n",
    "feats = i_classifier(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c62ac772-3025-4cd5-8539-7184489827f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9cd6817-fdcc-4ebd-b47f-caf64881df09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'conch_v1' in MODEL2CONSTANTS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4f17a-ff61-4a59-a365-abc5491c3c23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
